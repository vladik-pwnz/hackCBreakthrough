{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1545,
     "status": "ok",
     "timestamp": 1684550869013,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "BduI9gFcEsFq",
    "outputId": "775d8894-ba61-4c22-adcf-554672073a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available!  Training on GPU ...\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available.  Training on CPU ...')\n",
    "else:\n",
    "    print('CUDA is available!  Training on GPU ...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7071,
     "status": "ok",
     "timestamp": 1684550879045,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "HC2x1-bsExM2",
    "outputId": "1689eb8c-4bd9-432a-a8c6-09eb8a3ecb39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1684550879049,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "z2W_E8-_EsFt"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from skimage import io\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "from torchvision import transforms\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "\n",
    "from matplotlib import colors, pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=DeprecationWarning)\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "executionInfo": {
     "elapsed": 548,
     "status": "ok",
     "timestamp": 1684550885880,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "91D0rEkAEsFu"
   },
   "outputs": [],
   "source": [
    "DATA_MODES = ['train', 'val', 'test']\n",
    "RESCALE_SIZE = 224\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1684550890322,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "sGpH-bNIEsFv"
   },
   "outputs": [],
   "source": [
    "class Picture_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, files, mode):\n",
    "        super().__init__()\n",
    "        self.files = sorted(files)\n",
    "        self.mode = mode\n",
    "\n",
    "        if self.mode not in DATA_MODES:\n",
    "            print(f\"{self.mode} is not correct; correct modes: {DATA_MODES}\")\n",
    "            raise NameError\n",
    "\n",
    "        self.len_ = len(self.files)\n",
    "     \n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "        if self.mode != 'test':\n",
    "            self.labels = [path.parent.parent.name for path in self.files]\n",
    "            self.label_encoder.fit(self.labels)\n",
    "\n",
    "            with open('label_encoder.pkl', 'wb') as le_dump_file:\n",
    "                  pickle.dump(self.label_encoder, le_dump_file)\n",
    "                      \n",
    "    def __len__(self):\n",
    "        return self.len_\n",
    "      \n",
    "    def load_sample(self, file):\n",
    "        image = Image.open(file).convert('RGB')\n",
    "        image.load()\n",
    "        return image\n",
    "  \n",
    "    def __getitem__(self, index):\n",
    "        # для преобразования изображений в тензоры PyTorch и нормализации входа\n",
    "        transform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "        ])\n",
    "        x = self.load_sample(self.files[index])\n",
    "        x = self._prepare_sample(x)\n",
    "        x = np.array(x / 255, dtype='float32')\n",
    "        x = transform(x)\n",
    "        if self.mode == 'test':\n",
    "            return x\n",
    "        else:\n",
    "            label = self.labels[index]\n",
    "            label_id = self.label_encoder.transform([label])\n",
    "            y = label_id.item()\n",
    "            return x, y\n",
    "        \n",
    "    def _prepare_sample(self, image):\n",
    "        image = image.resize((RESCALE_SIZE, RESCALE_SIZE))\n",
    "        return np.array(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1684550891062,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "C_Ha40lDEsFw"
   },
   "outputs": [],
   "source": [
    "def imshow(inp, title=None, plt_ax=plt, default=False):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt_ax.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt_ax.set_title(title)\n",
    "    plt_ax.grid(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684550892667,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "F7KYXq8DEsFx"
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path('/content/drive/MyDrive/dataset')\n",
    "#TEST_DIR = Path('/test')\n",
    "#test_files = sorted(list(TEST_DIR.rglob('*.jpg')))\n",
    "\n",
    "frames_f = [Path('drive/MyDrive/dataset/klikun/images/' + i) for i in os.listdir(os.path.join(TRAIN_DIR, 'klikun/images'))]\n",
    "frames_f.extend([Path('drive/MyDrive/dataset/maliy/images/' + i) for i in os.listdir(os.path.join(TRAIN_DIR, 'maliy/images'))])\n",
    "frames_f.extend([Path('drive/MyDrive/dataset/shipun/images/' + i) for i in os.listdir(os.path.join(TRAIN_DIR, 'shipun/images'))])\n",
    "\n",
    "train_val_files = frames_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1684550894875,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "Dr7Cy4TAEsFx",
    "outputId": "58c1ae93-c4fe-4fc4-b540-ed64e5e15e10"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9038"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_val_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1684550896477,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "-8nQEvu_EsFz",
    "outputId": "6c623c98-5f94-43cd-ba1a-5282573b34c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7230 1808\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_val_labels = [path.parent.parent.name for path in train_val_files]\n",
    "train_files, val_files = train_test_split(train_val_files, test_size=0.20, stratify=train_val_labels, random_state=111)\n",
    "\n",
    "val_dataset = Picture_Dataset(val_files, mode='val')\n",
    "\n",
    "print(len(train_files), len(val_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1684550896479,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "vC4l4I3nEsF2"
   },
   "outputs": [],
   "source": [
    "train_dataset = Picture_Dataset(train_files, mode='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iWcwl4hEsF4"
   },
   "source": [
    "# Tuned Alexnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1684550899901,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "ZMZoN_MxEsF5"
   },
   "outputs": [],
   "source": [
    "class AlexNetV1(nn.Module):\n",
    "  \n",
    "    def __init__(self, n_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=3, out_channels=8, kernel_size=3),\n",
    "            nn.BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=8, out_channels=16, kernel_size=3),\n",
    "            nn.BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3),\n",
    "            nn.BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2),\n",
    "        )\n",
    "        \n",
    "        self.ln1 = nn.Sequential(\n",
    "            nn.Linear(128 * 5 * 5, 1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.Dropout(p=0.05),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.ln2 = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(p=0.05),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.ln3 = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.out = nn.Linear(128, n_classes)\n",
    "  \n",
    "  \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x=self.ln1(x)\n",
    "        x=self.ln2(x)\n",
    "        x=self.ln3(x)\n",
    "        \n",
    "        logits = self.out(x)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "executionInfo": {
     "elapsed": 811,
     "status": "ok",
     "timestamp": 1684550940546,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "PHrN1FUUEsF5"
   },
   "outputs": [],
   "source": [
    "def fit_epoch(model, train_loader, criterion, optimizer):\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_data = 0\n",
    "  \n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        preds = torch.argmax(outputs, 1)\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_data += inputs.size(0)\n",
    "              \n",
    "    train_loss = running_loss / processed_data\n",
    "    train_acc = running_corrects.cpu().numpy() / processed_data\n",
    "    return train_loss, train_acc\n",
    "\n",
    "\n",
    "def eval_epoch(model, val_loader, criterion):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "    processed_size = 0\n",
    "\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs = inputs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "\n",
    "        with torch.set_grad_enabled(False):\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            preds = torch.argmax(outputs, 1)\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "        processed_size += inputs.size(0)\n",
    "    val_loss = running_loss / processed_size\n",
    "    val_acc = running_corrects.double() / processed_size\n",
    "    return val_loss, val_acc\n",
    "\n",
    "\n",
    "def train(train_dataset, val_dataset, model, epochs, batch_size):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "    history = []\n",
    "    log_template = \"\\nEpoch {ep:03d} train_loss: {t_loss:0.4f} \\\n",
    "    val_loss {v_loss:0.4f} train_acc {t_acc:0.4f} val_acc {v_acc:0.4f}\"\n",
    "\n",
    "    with tqdm(desc=\"epoch\", total=epochs) as pbar_outer:\n",
    "        opt = torch.optim.Adam(model.parameters(), lr=7e-4, betas=(0.9, 0.99)) #добавлено для оптимизации\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            train_loss, train_acc = fit_epoch(model, train_loader, criterion, opt)\n",
    "            print(\"loss\", train_loss)\n",
    "            \n",
    "            val_loss, val_acc = eval_epoch(model, val_loader, criterion)\n",
    "            history.append((train_loss, train_acc, val_loss, val_acc))\n",
    "            \n",
    "            pbar_outer.update(1)\n",
    "            tqdm.write(log_template.format(ep=epoch+1, t_loss=train_loss,\\\n",
    "                                           v_loss=val_loss, t_acc=train_acc, v_acc=val_acc))\n",
    "\n",
    "            torch.save(model.state_dict(), SAVE_W_PATH + f'model_weights_{epoch}.pth')\n",
    "            \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1684550904858,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "Zi3QkSrXEsF6"
   },
   "outputs": [],
   "source": [
    "def predict(model, test_loader):\n",
    "    with torch.no_grad():\n",
    "        logits = []\n",
    "    \n",
    "        for inputs in test_loader:\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            model.eval()\n",
    "            outputs = model(inputs).cpu()\n",
    "            logits.append(outputs)\n",
    "            \n",
    "    probs = nn.functional.softmax(torch.cat(logits), dim=-1).numpy()\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1684550908303,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "SDobvucxEsF6",
    "outputId": "4adc1d9f-bd6d-49fd-9ed6-605c1fd0a514"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we will classify :3\n",
      "AlexNetV1(\n",
      "  (conv1): Sequential(\n",
      "    (0): Conv2d(3, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv2): Sequential(\n",
      "    (0): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv3): Sequential(\n",
      "    (0): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv4): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (conv5): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (ln1): Sequential(\n",
      "    (0): Linear(in_features=3200, out_features=1024, bias=True)\n",
      "    (1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.05, inplace=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (ln2): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): Dropout(p=0.05, inplace=False)\n",
      "    (3): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (ln3): Sequential(\n",
      "    (0): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.01)\n",
      "  )\n",
      "  (out): Linear(in_features=128, out_features=3, bias=True)\n",
      ")\n",
      "['klikun' 'maliy' 'shipun']\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(np.unique(train_val_labels))\n",
    "simple_cnn = AlexNetV1(n_classes).to(DEVICE)\n",
    "print(\"we will classify :{}\".format(n_classes))\n",
    "print(simple_cnn)\n",
    "print(np.unique(train_val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "executionInfo": {
     "elapsed": 553,
     "status": "ok",
     "timestamp": 1684550948168,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "qW2T-b1sEsF7"
   },
   "outputs": [],
   "source": [
    "if val_dataset is None:\n",
    "    val_dataset = Picture_Dataset(val_files, mode='val')\n",
    "    \n",
    "if train_dataset is None:\n",
    "    train_dataset = Picture_Dataset(train_files, mode='train')\n",
    "\n",
    "SAVE_W_PATH = '/content/drive/MyDrive/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARmVyfzeEsF8",
    "outputId": "fc129254-06f8-4644-dc5e-4b26ef9e96f2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "epoch:   0%|          | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.9208919671890976\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:   7%|▋         | 1/15 [04:50<1:07:45, 290.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 001 train_loss: 0.9209     val_loss 0.8509 train_acc 0.5604 val_acc 0.5951\n",
      "loss 0.752745250514599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch:  13%|█▎        | 2/15 [09:41<1:02:57, 290.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 002 train_loss: 0.7527     val_loss 0.6711 train_acc 0.6603 val_acc 0.7035\n"
     ]
    }
   ],
   "source": [
    "history = train(train_dataset, val_dataset, model=simple_cnn, epochs=6, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1684549272108,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "lHk-OG5s9FrH"
   },
   "outputs": [],
   "source": [
    "torch.save(simple_cnn.state_dict(), '/content/drive/MyDrive/model_test_save.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 568
    },
    "executionInfo": {
     "elapsed": 1084,
     "status": "ok",
     "timestamp": 1684550553881,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "xHaXiv2XEsF8",
    "outputId": "eb20cadf-4251-46a1-c734-e2709243f5b9"
   },
   "outputs": [],
   "source": [
    "loss, acc, val_loss, val_acc = zip(*history)\n",
    "\n",
    "plt.figure(figsize=(15, 9))\n",
    "plt.plot(loss, label=\"train_loss\")\n",
    "plt.plot(val_loss, label=\"val_loss\")\n",
    "plt.legend(loc='best')\n",
    "plt.xlabel(\"epochs\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "executionInfo": {
     "elapsed": 19797,
     "status": "ok",
     "timestamp": 1684550591693,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "0C51cm1oEsF9"
   },
   "outputs": [],
   "source": [
    "def predict_one_sample(model, inputs, device=DEVICE):\n",
    "    with torch.no_grad():\n",
    "        inputs = inputs.to(device)\n",
    "        model.eval()\n",
    "        logit = model(inputs).cpu()\n",
    "        probs = torch.nn.functional.softmax(logit, dim=-1).numpy()\n",
    "    return probs\n",
    "\n",
    "random_characters = int(np.random.uniform(0,1000))\n",
    "ex_img, true_label = val_dataset[random_characters]\n",
    "probs_im = predict_one_sample(simple_cnn, ex_img.unsqueeze(0))\n",
    "\n",
    "idxs = list(map(int, np.random.uniform(0,1000, 300)))\n",
    "imgs = [val_dataset[id][0].unsqueeze(0) for id in idxs]\n",
    "\n",
    "probs_ims = predict(simple_cnn, imgs)\n",
    "\n",
    "label_encoder = pickle.load(open(\"label_encoder.pkl\", 'rb'))\n",
    "\n",
    "y_pred = np.argmax(probs_ims,-1)\n",
    "\n",
    "actual_labels = [val_dataset[id][1] for id in idxs]\n",
    "\n",
    "preds_class = [label_encoder.classes_[i] for i in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1013,
     "status": "ok",
     "timestamp": 1684550609045,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "3H3iP4gSEsF9",
    "outputId": "ebe40f3f-8118-4ec9-de0b-bd6befe6825b"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "f1_score(actual_labels, y_pred, average='micro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 929
    },
    "executionInfo": {
     "elapsed": 9643,
     "status": "ok",
     "timestamp": 1684550626189,
     "user": {
      "displayName": "Andrew Fadeev",
      "userId": "06604953660168675696"
     },
     "user_tz": -180
    },
    "id": "B6LmlXFsEsF9",
    "outputId": "a323b301-38f1-4ea9-f78c-d90dc3cf1e4f"
   },
   "outputs": [],
   "source": [
    "import matplotlib.patches as patches\n",
    "from matplotlib.font_manager import FontProperties\n",
    "\n",
    "fig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n",
    "                        sharey=True, sharex=True)\n",
    "for fig_x in ax.flatten():\n",
    "    random_characters = int(np.random.uniform(0,1000))\n",
    "    im_val, label = val_dataset[random_characters]\n",
    "    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n",
    "                val_dataset.label_encoder.inverse_transform([label])[0].split('_')))\n",
    "    \n",
    "    \n",
    "\n",
    "    imshow(im_val.data.cpu(), \\\n",
    "          title=img_label,plt_ax=fig_x)\n",
    "    \n",
    "    actual_text = \"Actual : {}\".format(img_label)\n",
    "            \n",
    "    fig_x.add_patch(patches.Rectangle((0, 53),86,35,color='white'))\n",
    "    font0 = FontProperties()\n",
    "    font = font0.copy()\n",
    "    font.set_family(\"fantasy\")\n",
    "    prob_pred = predict_one_sample(simple_cnn, im_val.unsqueeze(0))\n",
    "    predicted_proba = np.max(prob_pred)*100\n",
    "    y_pred = np.argmax(prob_pred)\n",
    "    \n",
    "    predicted_label = label_encoder.classes_[y_pred]\n",
    "    predicted_label = predicted_label[:len(predicted_label)//2] + '\\n' + predicted_label[len(predicted_label)//2:]\n",
    "    predicted_text = \"{} : {:.0f}%\".format(predicted_label,predicted_proba)\n",
    "            \n",
    "    fig_x.text(1, 59, predicted_text , horizontalalignment='left', fontproperties=font,\n",
    "                    verticalalignment='top',fontsize=8, color='black',fontweight='bold')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YbOmfONPEsF-"
   },
   "outputs": [],
   "source": [
    "test_dataset = Picture_Dataset(test_files, mode=\"test\")\n",
    "test_loader = DataLoader(test_dataset, shuffle=False, batch_size=64)\n",
    "probs = predict(simple_cnn, test_loader)\n",
    "\n",
    "preds = label_encoder.inverse_transform(np.argmax(probs, axis=1))\n",
    "test_filenames = [path.name for path in test_dataset.files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDZ6WrgAEsF-"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "my_submit = pd.DataFrame({'Id': test_filenames, 'Expected': preds})\n",
    "my_submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tm0j1aF1EsF-"
   },
   "outputs": [],
   "source": [
    "my_submit.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
