{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "from pathbook.pathbook import *\n",
    "labels = ['klikun', 'maliy', 'shipun']\n",
    "labeldict = {'klikun':0, 'maliy':1, 'shipun':2}\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### annotating initial dataset for detection and segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: ../data/train: File exists\n",
      "mkdir: ../data/val: File exists\n"
     ]
    }
   ],
   "source": [
    "# prepare directories for classification\n",
    "!mkdir ../data/train\n",
    "!mkdir ../data/train/klikun\n",
    "!mkdir ../data/train/maliy\n",
    "!mkdir ../data/train/shipun\n",
    "\n",
    "!mkdir ../data/val\n",
    "!mkdir ../data/val/klikun\n",
    "!mkdir ../data/val/maliy\n",
    "!mkdir ../data/val/shipun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path_test_annotation)\n",
    "for img in df['path'].unique():\n",
    "    with open(os.path.join(path_test_dataset,img.replace('.jpg','.txt')),'w') as txt:\n",
    "        info = df[df['path']==img].copy()\n",
    "        info['x']=(info.x_min+info.x_max)/2\n",
    "        info['y']=(info.y_min+info.y_max)/2\n",
    "        info['w']=(-info.x_min+info.x_max)\n",
    "        info['h']=(-info.y_min+info.y_max)\n",
    "        for idx, row in info.iterrows():\n",
    "            print(labeldict[row.label],row.x,row.y,row.w,row.h,file=txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f ../data/test/*.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2711/2711 [00:17<00:00, 152.22it/s]\n",
      "100%|██████████| 15357/15357 [01:44<00:00, 147.12it/s]\n"
     ]
    }
   ],
   "source": [
    "for _ in [path_val_dataset, path_train_dataset]:\n",
    "    for img in tqdm(os.listdir(_)):\n",
    "        if img[-4:]=='.jpg':\n",
    "            label = img.split('-')[1]\n",
    "            cv2.imwrite(os.path.join(_,label,img), cv2.imread(os.path.join(_,img)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [01:22<00:00, 36.70it/s]\n",
      "100%|██████████| 3002/3002 [00:54<00:00, 54.61it/s]\n",
      "100%|██████████| 3011/3011 [01:57<00:00, 25.70it/s]\n"
     ]
    }
   ],
   "source": [
    "columns=['set','path','class_name','class_id','x_min','y_min','x_max','y_max']\n",
    "train_array = []\n",
    "val_array = []\n",
    "\n",
    "np.random.seed(17)\n",
    "val_p = 0.15 \n",
    "\n",
    "# iterate through the initial formated for classification dataset\n",
    "dataset = path_initial_train_dataset\n",
    "for label_idx, label in enumerate(labels):\n",
    "    mask_dir = os.path.join(dataset, label, 'masks')\n",
    "    image_dir = os.path.join(dataset, label, 'images')\n",
    "\n",
    "    for file_idx, mask in enumerate(tqdm(os.listdir(mask_dir))):\n",
    "        impath = os.path.join(image_dir, mask.replace('.png','.jpg'))\n",
    "        if not os.path.exists(impath):\n",
    "            # print(impath)\n",
    "            continue # drop masks without images\n",
    "\n",
    "        image = cv2.imread(impath)\n",
    "        mask = cv2.imread(os.path.join(mask_dir, mask))\n",
    "\n",
    "        h, w, _ = image.shape\n",
    "        h_, w_, _ = mask.shape\n",
    "        if not (h == h_ and w == w_):\n",
    "            continue # drop images with uncorrest masks\n",
    "\n",
    "        save_dir=''\n",
    "        save_cls_dir=''\n",
    "        val = (np.random.random() < val_p)\n",
    "        if val:\n",
    "            save_dir = path_val_dataset\n",
    "            save_cls_dir = path_cls_val_dataset\n",
    "        else:\n",
    "            save_dir = path_train_dataset\n",
    "            save_cls_dir = path_cls_train_dataset\n",
    "\n",
    "        # name format: \"[scource: initial|extra|augmented]-[label: klikun|maliy|shipun]-[ingroup index]]\"\n",
    "        name = f'initial-{label}-{file_idx}'\n",
    "        path = os.path.join(save_dir, label, name)\n",
    "\n",
    "        # saving image and annotation for segmentation task\n",
    "        cv2.imwrite(path+'.jpg', image)\n",
    "        with open(path+'.txt','w') as labelfile:\n",
    "\n",
    "            imgray = cv2.cvtColor(mask, cv2.COLOR_BGR2GRAY)\n",
    "            colors = np.unique(imgray)[1:]\n",
    "            for color_idx, color in enumerate(colors):\n",
    "                #choose one color (one object) from mask and find contours\n",
    "                _, thresh = cv2.threshold(imgray, color, color, type=cv2.THRESH_TOZERO_INV)\n",
    "                _, thresh = cv2.threshold(thresh, color-1, color, type=cv2.THRESH_TOZERO)\n",
    "                contours, _ = cv2.findContours(thresh, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "                \n",
    "                superseg = []\n",
    "                for contour in contours:\n",
    "                    seg = (contour[:,0]/[w,h]).flatten().tolist()\n",
    "                    superseg += seg\n",
    "                    # if mode=='seg':\n",
    "                    print(0,*seg,file=labelfile) # save each spot for segmentation task\n",
    "\n",
    "                # contours coords\n",
    "                x = superseg[0::2]\n",
    "                x_min = min(x)\n",
    "                x_max = max(x)\n",
    "\n",
    "                y = superseg[1::2]\n",
    "                y_min = min(y)\n",
    "                y_max = max(y)\n",
    "\n",
    "                w_ = x_max-x_min\n",
    "                h_ = y_max-y_min\n",
    "\n",
    "                # saving different crops for classification task\n",
    "                cls_path = os.path.join(save_cls_dir, label, name+f\"-{color_idx}\")\n",
    "                cls_a_path = os.path.join(save_cls_dir, label, f'augmented-{label}-{file_idx}-{color_idx}')\n",
    "                cv2.imwrite(cls_a_path+'-1'+'.jpg', \n",
    "                            image[int(y_min*h) : int(y_max*h), \n",
    "                                  int(x_min*w) : int(x_max*w)])\n",
    "                if w_ < 0.8 and h_ < 0.8:\n",
    "                    cv2.imwrite(cls_path+'-1.2'+'.jpg', \n",
    "                            image[int(max((y_min-0.1*h_),0)*h) : int(min((y_max+0.1*h_),1)*h), \n",
    "                                  int(max((x_min-0.1*w_),0)*w) : int(min((x_max+0.1*w_),1)*w)])\n",
    "                if w_ < 0.6 and h_ < 0.6:\n",
    "                    cv2.imwrite(cls_a_path+'-1.4'+'.jpg', \n",
    "                            image[int(max((y_min-0.2*h_),0)*h) : int(min((y_max+0.2*h_),1)*h), \n",
    "                                  int(max((x_min-0.2*w_),0)*w) : int(min((x_max+0.2*w_),1)*w)])\n",
    "\n",
    "                # if mode=='det':\n",
    "                #     print(label_idx,min(x),min(y),max(x),max(y)) # save bboxes for detection\n",
    "\n",
    "                if val:\n",
    "                    val_array.append(['val',path+'.jpg',label,labeldict[label],x_min,y_min,x_max,y_max])\n",
    "                else:\n",
    "                    train_array.append(['train',path+'.jpg',label,labeldict[label],x_min,y_min,x_max,y_max])\n",
    "\n",
    "# if mode!='cls': #save annotations for detection\n",
    "pd.DataFrame(train_array, columns=columns).to_csv(path_train_annotation,index=False)\n",
    "pd.DataFrame(val_array, columns=columns).to_csv(path_val_annotation,index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
